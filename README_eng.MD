Hereâ€™s the English version of the **README.md** for your project:

---

# ğŸ—£ï¸ Parquet Englishâ€“Indonesian Translator

This script is used to **translate `.parquet` dataset files** containing English text into **Indonesian**, using the **Hugging Face model `Helsinki-NLP/opus-mt-en-id`**.

## ğŸ“‹ Project Description

The script processes all `.parquet` files in an input folder, translates text columns such as `question` and `multiple_choice_answer`, and saves the results into a new output folder in `.parquet` format.
Translation is performed in **batches** with real-time progress tracking using `tqdm`, making the process efficient and easy to monitor.

---

## âš™ï¸ Key Features

* âœ… Supports **batch translation** (20 rows per iteration) for efficiency
* âœ… Uses **Transformer-based translation model** (`Helsinki-NLP/opus-mt-en-id`)
* âœ… Works with both **CPU and GPU** (`--use_gpu` flag)
* âœ… Automatically processes all `.parquet` files in a directory
* âœ… Displays real-time progress via `tqdm`

---

## ğŸ§  Logical Workflow

```mermaid
flowchart TD
    A["Input Folder (.parquet files)"] --> B["Read Parquet File (Pandas)"]
    B --> C["Extract 'question' and 'multiple_choice_answer' Columns"]
    C --> D["Translate in Batches of 20 Rows (HF pipeline)"]
    D --> E["Save Translated File (.parquet)"]
    E --> F["Output Folder with Translated Files"]
```

---

## ğŸ§° Installation

Set up your Python virtual environment and install the required dependencies:

```bash
pip install torch transformers pandas tqdm pyarrow
```

---

## ğŸš€ How to Run

Execute the script with the following command:

```bash
python terjemahversi3.py \
  --input_dir "/path/to/input_folder" \
  --output_dir "./vqav2_translated_parquet" \
  --use_gpu
```

### Argument Description

| Argument       | Required | Description                                               |
| -------------- | -------- | --------------------------------------------------------- |
| `--input_dir`  | âœ…        | Directory containing original `.parquet` files in English |
| `--output_dir` | âœ…        | Destination folder for translated output files            |
| `--use_gpu`    | Optional | Enables GPU acceleration if available (default: CPU)      |

Example execution:

```bash
(venvtrainingllm) openai@openai:~/trainingllm/dataset/dataset$ python terjemahversi3.py \
  --input_dir "/home/openai/.cache/huggingface/hub/datasets--merve--vqav2-small/snapshots/4b070c6254225a7355070c94e14c3275606f521d/data" \
  --output_dir "./vqav2_translated_parquet" \
  --use_gpu
```

---

## ğŸ“‚ Folder Structure

```
dataset/
â”œâ”€â”€ terjemahversi3.py
â”œâ”€â”€ vqav2_translated_parquet/
â”‚   â”œâ”€â”€ file1.parquet
â”‚   â”œâ”€â”€ file2.parquet
â”‚   â””â”€â”€ ...
â””â”€â”€ original_data/
    â”œâ”€â”€ file1.parquet
    â”œâ”€â”€ file2.parquet
    â””â”€â”€ ...
```

---

## ğŸ§© Main Functions

| Function                   | Description                                                                              |
| -------------------------- | ---------------------------------------------------------------------------------------- |
| `translate_batch()`        | Translates a list of text strings in batches using the Hugging Face translation pipeline |
| `translate_parquet_file()` | Reads a `.parquet` file, translates specific columns, and saves the output               |
| `main()`                   | Parses CLI arguments and processes all `.parquet` files in the input directory           |

---

## âš¡ Notes

* Use `--use_gpu` for faster performance when translating large datasets.
* The model will be automatically downloaded from Hugging Face on first use.
* If you encounter the error `CUDA not available`, simply run the script without `--use_gpu` to fall back to CPU.

---

## ğŸ“œ License

This project uses the open translation model [Helsinki-NLP/opus-mt-en-id](https://huggingface.co/Helsinki-NLP/opus-mt-en-id) from Hugging Face and is intended for **research and non-commercial use**.

---

Would you like me to add a **â€œSample Inputâ€“Output Translation Exampleâ€** section next? It can help make the README more practical for readers on GitHub.
